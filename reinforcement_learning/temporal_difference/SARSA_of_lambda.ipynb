{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def epsilon_greedy(Q, state, epsilon):\n",
    "    \"\"\"                                                                                                                 \n",
    "    Uses epsilon-greedy to determine if the reinforcement learning is                                                   \n",
    "       exploring or exploiting and uses to get action                                                                   \n",
    "                                                                                                                        \n",
    "    parameters:                                                                                                         \n",
    "        Q [numpy.ndarray of shape (s, a)]: contains the Q table                                                         \n",
    "        state: the current state                                                                                        \n",
    "        epsilon: the threshold for epsilon-greedy                                                                       \n",
    "                                                                                                                        \n",
    "    returns:                                                                                                            \n",
    "        the action to take                                                                                              \n",
    "    \"\"\"\n",
    "    # determine exploring-exploiting balance by comparing to epsilon                                                    \n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        # exploring                                                                                                     \n",
    "        action = np.random.randint(Q.shape[1])\n",
    "    else:\n",
    "        # exploiting                                                                                                    \n",
    "        action = np.argmax(Q[state, :])\n",
    "    return action\n",
    "\n",
    "def sarsa_lambtha(env, Q, lambtha, episodes=5000, max_steps=100, alpha=0.1,\n",
    "                  gamma=0.99, epsilon=1, min_epsilon=0.1, epsilon_decay=0.05):\n",
    "    \"\"\"                                                                                                                 \n",
    "    Performs the SARSA(Î») algorithm                                                                                     \n",
    "                                                                                                                        \n",
    "    parameters:                                                                                                         \n",
    "        env: the openAI environment instance                                                                            \n",
    "        Q [numpy.ndarray of shape(s, a)]: contains the Q table                                                          \n",
    "        lambtha: the eligibility trace factor                                                                           \n",
    "        episodes [int]: total number of episodes to train over                                                          \n",
    "        max_steps [int]: the maximum number of steps per episode                                                        \n",
    "        alpha [float]: the learning rate                                                                                \n",
    "        gamma [float]: the discount rate                                                                                \n",
    "        epsilon: the initial threshold for epsilon greedy                                                               \n",
    "        min_epsilon [float]: the minimum value that epsilon should decay to                                             \n",
    "        epsilon_decay [float]: decay rate for updating epsilon between episodes                                         \n",
    "                                                                                                                        \n",
    "    returns:                                                                                                            \n",
    "        Q: the updated Q table                                                                                          \n",
    "    \"\"\"\n",
    "    # set maximum epsilon to the current epsilon before epsilon_decay                                                   \n",
    "    max_epsilon = epsilon\n",
    "    # Sets the eligibility traces to numpy array of zeros of same shape as Q                                            \n",
    "    Et = np.zeros((Q.shape))\n",
    "    # iterate over all episodes                                                                                         \n",
    "    for ep in range(episodes):\n",
    "        # set the initial state of each episode to environment reset                                                    \n",
    "        state = env.reset()\n",
    "        # get the action from epsilon-greedy function                                                                   \n",
    "        action = epsilon_greedy(Q, state, epsilon)\n",
    "        # iterate up to maximum number of steps per episode                                                             \n",
    "        for step in range(max_steps):\n",
    "            # eligibility traces updated with lambda & gamma                                                            \n",
    "            Et = Et * lambtha * gamma\n",
    "            # increase Et for current state, action                                                                     \n",
    "            Et[state, action] += 1\n",
    "\n",
    "            # perform the action to get next_state, reward, done, and info                                              \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            # update the action, using epsilon-greedy again                                                             \n",
    "            next_action = epsilon_greedy(Q, state, epsilon)\n",
    "\n",
    "            # if the algorithm finds a hole, the reward is updated to -1                                                \n",
    "            if env.desc.reshape(env.observation_space.n)[next_state] == b'H':\n",
    "                reward = -1\n",
    "            # if the algorithm finds the goal, the reward is updated to 1                                               \n",
    "            if env.desc.reshape(env.observation_space.n)[next_state] == b'G':\n",
    "                reward = 1\n",
    "\n",
    "            # calculate delta_t                                                                                         \n",
    "            # delta_t = R(t + 1) + gamma * Q(St + 1, At + 1) - Q(St, At)                                                \n",
    "            delta_t = reward + (\n",
    "                gamma * Q[next_state, next_action]) - Q[state, action]\n",
    "            # upddate Q table                                                                                           \n",
    "            # Q(st) = Q(st) + alpha * delta_t * Et(St)                                                                  \n",
    "            Q[state, action] = Q[state, action] + (\n",
    "                alpha * delta_t * Et[state, action])\n",
    "            # if done, break out of episode                                                                             \n",
    "            if done:\n",
    "                break\n",
    "            # otherwise, reset state, action and continue                                                               \n",
    "            state = next_state\n",
    "            action = next_action\n",
    "        # after each epsiode, update epsilon to decay                                                                   \n",
    "        # epsilon will now favor slightly more exploitation than exploration                                            \n",
    "        epsilon = min_epsilon + (\n",
    "            (max_epsilon - min_epsilon) * np.exp(-epsilon_decay * ep))\n",
    "    # when all episodes completed, return updated Q table                                                               \n",
    "    return Q\n",
    "\n",
    "np.random.seed(0)\n",
    "env = gym.make('FrozenLake8x8-v0')\n",
    "Q = np.random.uniform(size=(64, 4))\n",
    "np.set_printoptions(precision=4)\n",
    "print(sarsa_lambtha(env, Q, 0.9))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
