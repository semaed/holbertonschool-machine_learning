{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_episode(env, policy, max_steps):\n",
    "    \"\"\"                                                                                                                 \n",
    "    Generates an episode using policy                                                                                   \n",
    "                                                                                                                        \n",
    "    parameters:                                                                                                         \n",
    "        env: the openAI environment instance                                                                            \n",
    "        policy: function that takes in state & returns the next action to take                                          \n",
    "        max_steps: the maximum number of steps per episode                                                              \n",
    "                                                                                                                        \n",
    "    returns:                                                                                                            \n",
    "        returns the episode                                                                                             \n",
    "    \"\"\"\n",
    "    episode = [[], []]\n",
    "    state = env.reset()\n",
    "    for step in range(max_steps):\n",
    "        action = policy(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        episode[0].append(state)\n",
    "\n",
    "        if env.desc.reshape(env.observation_space.n)[next_state] == b'H':\n",
    "            episode[1].append(-1)\n",
    "            return episode\n",
    "        if env.desc.reshape(env.observation_space.n)[next_state] == b'G':\n",
    "            episode[1].append(1)\n",
    "            return episode\n",
    "        episode[1].append(0)\n",
    "        state = next_state\n",
    "    return episode\n",
    "\n",
    "def monte_carlo(env, V, policy, episodes=5000, max_steps=100,\n",
    "                alpha=0.1, gamma=0.99):\n",
    "    \"\"\"                                                                                                                 \n",
    "    Performs the Monte Carlo algorithm                                                                                  \n",
    "                                                                                                                        \n",
    "    parameters:                                                                                                         \n",
    "        env: the openAI environment instance                                                                            \n",
    "        V [numpy.ndarray of shape(s,)]: contains the value estimate                                                     \n",
    "        policy: function that takes in state & returns the next action to take                                          \n",
    "        episodes [int]: total number of episodes to train over                                                          \n",
    "        max_steps [int]: the maximum number of steps per episode                                                        \n",
    "        alpha [float]: the learning rate                                                                                \n",
    "        gamma [float]: the discount rate                                                                                \n",
    "                                                                                                                        \n",
    "    returns:                                                                                                            \n",
    "        V: the updated value estimate                                                                                   \n",
    "    \"\"\"\n",
    "    discounts = np.array([gamma ** i for i in range(max_steps)])\n",
    "    for ep in range(episodes):\n",
    "        episode = generate_episode(env, policy, max_steps)\n",
    "\n",
    "        for i in range(len(episode[0])):\n",
    "            Gt = np.sum(np.array(episode[1][i:]) *\n",
    "                        np.array(discounts[:len(episode[1][i:])]))\n",
    "            V[episode[0][i]] = (V[episode[0][i]] +\n",
    "                                alpha * (Gt - V[episode[0][i]]))\n",
    "    return V\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "env = gym.make('FrozenLake8x8-v0')\n",
    "LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
    "\n",
    "def policy(s):\n",
    "    p = np.random.uniform()\n",
    "    if p > 0.5:\n",
    "        if s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
    "            return RIGHT\n",
    "        elif s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
    "            return DOWN\n",
    "        elif s // 8 != 0 and env.desc[s // 8 - 1, s % 8] != b'H':\n",
    "            return UP\n",
    "        else:\n",
    "            return LEFT\n",
    "    else:\n",
    "        if s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
    "            return DOWN\n",
    "        elif s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
    "            return RIGHT\n",
    "        elif s % 8 != 0 and env.desc[s // 8, s % 8 - 1] != b'H':\n",
    "            return LEFT\n",
    "        else:\n",
    "            return UP\n",
    "\n",
    "V = np.where(env.desc == b'H', -1, 1).reshape(64).astype('float64')\n",
    "np.set_printoptions(precision=2)\n",
    "env.seed(0)\n",
    "print(monte_carlo(env, V, policy).reshape((8, 8)))\n",
    "\n",
    "     \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
