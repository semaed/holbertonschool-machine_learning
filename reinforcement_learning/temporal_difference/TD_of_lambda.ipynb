{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def td_lambtha(env, V, policy, lambtha, episodes=5000, max_steps=100,\n",
    "               alpha=0.1, gamma=0.99):\n",
    "    \"\"\"                                                                                                                 \n",
    "    Performs the TD(Î») algorithm                                                                                        \n",
    "                                                                                                                        \n",
    "    parameters:                                                                                                         \n",
    "        env: the openAI environment instance                                                                            \n",
    "        V [numpy.ndarray of shape(s,)]: contains the value estimate                                                     \n",
    "        policy: function that takes in state & returns the next action to take                                          \n",
    "        episodes [int]: total number of episodes to train over                                                          \n",
    "        max_steps [int]: the maximum number of steps per episode                                                        \n",
    "        alpha [float]: the learning rate                                                                                \n",
    "        gamma [float]: the discount rate                                                                                \n",
    "                                                                                                                        \n",
    "    returns:                                                                                                            \n",
    "        V: the updated value estimate                                                                                   \n",
    "    \"\"\"\n",
    "    episode = [[], []]\n",
    "    Et = [0 for i in range(env.observation_space.n)]\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        for step in range(max_steps):\n",
    "            Et = list(np.array(Et) * lambtha * gamma)\n",
    "            Et[state] += 1\n",
    "\n",
    "            action = policy(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            if env.desc.reshape(env.observation_space.n)[next_state] == b'H':\n",
    "                reward = -1\n",
    "\n",
    "            if env.desc.reshape(env.observation_space.n)[next_state] == b'G':\n",
    "                reward = 1\n",
    "\n",
    "            delta_t = reward + gamma * V[next_state] - V[state]\n",
    "\n",
    "            V[state] = V[state] + alpha * delta_t * Et[state]\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "    return np.array(V)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "env = gym.make('FrozenLake8x8-v0')\n",
    "LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
    "\n",
    "def policy(s):\n",
    "    p = np.random.uniform()\n",
    "    if p > 0.5:\n",
    "        if s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
    "            return RIGHT\n",
    "        elif s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
    "            return DOWN\n",
    "        elif s // 8 != 0 and env.desc[s // 8 - 1, s % 8] != b'H':\n",
    "            return UP\n",
    "        else:\n",
    "            return LEFT\n",
    "    else:\n",
    "        if s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
    "            return DOWN\n",
    "        elif s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
    "            return RIGHT\n",
    "        elif s % 8 != 0 and env.desc[s // 8, s % 8 - 1] != b'H':\n",
    "            return LEFT\n",
    "        else:\n",
    "            return UP\n",
    "\n",
    "V = np.where(env.desc == b'H', -1, 1).reshape(64).astype('float64')\n",
    "np.set_printoptions(precision=4)\n",
    "print(td_lambtha(env, V, policy, 0.9).reshape((8, 8)))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
