{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open In Colab\n",
    "\n",
    "%tensorflow_version 2.x\n",
    "!pip install tensorflow-hub\n",
    "!pip install transformers\n",
    "     \n",
    "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
    "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (1.19.5)\n",
    "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-hub) (3.12.4)\n",
    "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow-hub) (57.0.0)\n",
    "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow-hub) (1.15.0)\n",
    "Collecting transformers\n",
    "  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
    "     |████████████████████████████████| 2.3MB 4.1MB/s \n",
    "Collecting sacremoses\n",
    "  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
    "     |████████████████████████████████| 901kB 36.1MB/s \n",
    "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
    "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
    "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
    "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
    "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
    "Collecting tokenizers<0.11,>=0.10.1\n",
    "  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
    "     |████████████████████████████████| 3.3MB 39.2MB/s \n",
    "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
    "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
    "Collecting huggingface-hub==0.0.8\n",
    "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
    "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
    "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
    "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
    "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
    "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
    "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
    "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
    "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
    "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
    "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
    "Installing collected packages: sacremoses, tokenizers, huggingface-hub, transformers\n",
    "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "def question_answer(question, reference):\n",
    "    \"\"\"                                                                                                                 \n",
    "    Finds a snippet of text within a reference document to answer a question                                            \n",
    "                                                                                                                        \n",
    "    parameters:                                                                                                         \n",
    "        question [string]:                                                                                              \n",
    "            contains the question to answer                                                                             \n",
    "        reference [string]:                                                                                             \n",
    "            contains the reference document from which to find the answer                                               \n",
    "                                                                                                                        \n",
    "    returns:                                                                                                            \n",
    "        [string]:                                                                                                       \n",
    "            contains the answer                                                                                         \n",
    "        or None if no answer is found                                                                                   \n",
    "    \"\"\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(\n",
    "        'bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "    model = hub.load(\"https://tfhub.dev/see--/bert-uncased-tf2-qa/1\")\n",
    "\n",
    "    quest_tokens = tokenizer.tokenize(question)\n",
    "    refer_tokens = tokenizer.tokenize(reference)\n",
    "\n",
    "    tokens = ['[CLS]'] + quest_tokens + ['[SEP]'] + refer_tokens + ['[SEP]']\n",
    "\n",
    "    input_word_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1] * len(input_word_ids)\n",
    "    input_type_ids = [0] * (\n",
    "        1 + len(quest_tokens) + 1) + [1] * (len(refer_tokens) + 1)\n",
    "\n",
    "    input_word_ids, input_mask, input_type_ids = map(\n",
    "        lambda t: tf.expand_dims(\n",
    "            tf.convert_to_tensor(t, dtype=tf.int32), 0),\n",
    "        (input_word_ids, input_mask, input_type_ids))\n",
    "\n",
    "    outputs = model([input_word_ids, input_mask, input_type_ids])\n",
    "\n",
    "    short_start = tf.argmax(outputs[0][0][1:]) + 1\n",
    "    short_end = tf.argmax(outputs[1][0][1:]) + 1\n",
    "    answer_tokens = tokens[short_start: short_end + 1]\n",
    "    answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "\n",
    "\n",
    "    if answer is None or answer is \"\" or question in answer:\n",
    "      return None\n",
    "\n",
    "    return answer\n",
    "\n",
    "with open('drive/MyDrive/ZendeskArticles/PeerLearningDays.md') as f:\n",
    "    reference = f.read()\n",
    "\n",
    "print(question_answer('jherouhfuoernfnerjnfrn?', reference))\n",
    "print(question_answer('When are PLDs?', reference))\n",
    "print(question_answer('What is a potato?', reference))\n",
    "     \n",
    "jherouhfuoernfnerjnfrn ?\n",
    "on - site days from 9 : 00 am to 3 : 00 pm\n",
    "what is a potato ?\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "def answer_loop(reference):\n",
    "    \"\"\"                                                                                                                 \n",
    "    Answers questions from a reference text on loop (based on 1-loop.py)                                                \n",
    "                                                                                                                        \n",
    "    parameters:                                                                                                         \n",
    "        reference [string]:                                                                                             \n",
    "            the reference document from which to find the answer                                                        \n",
    "    \"\"\"\n",
    "    while (1):\n",
    "        user_input = input(\"Q: \")\n",
    "        user_input = user_input.lower()\n",
    "        if user_input == 'exit' or user_input == 'quit' \\\n",
    "           or user_input == 'goodbye' or user_input == 'bye':\n",
    "            print(\"A: Goodbye\")\n",
    "            break\n",
    "        answer = question_answer(user_input, reference)\n",
    "        if answer is None:\n",
    "            print(\"A: Sorry, I do not understand your question.\")\n",
    "        else:\n",
    "            print(\"A: \", answer)\n",
    "\n",
    "\n",
    "def question_answer(question, reference):\n",
    "    \"\"\"                                                                                                                 \n",
    "    Finds a snippet of text within a reference document to answer a question                                            \n",
    "                                                                                                                        \n",
    "    parameters:                                                                                                         \n",
    "        question [string]:                                                                                              \n",
    "            contains the question to answer                                                                             \n",
    "        reference [string]:                                                                                             \n",
    "            contains the reference document from which to find the answer                                               \n",
    "                                                                                                                        \n",
    "    returns:                                                                                                            \n",
    "        [string]:                                                                                                       \n",
    "            contains the answer                                                                                         \n",
    "        or None if no answer is found                                                                                   \n",
    "    \"\"\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(\n",
    "        'bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "    model = hub.load(\"https://tfhub.dev/see--/bert-uncased-tf2-qa/1\")\n",
    "\n",
    "    quest_tokens = tokenizer.tokenize(question)\n",
    "    refer_tokens = tokenizer.tokenize(reference)\n",
    "\n",
    "    tokens = ['[CLS]'] + quest_tokens + ['[SEP]'] + refer_tokens + ['[SEP]']\n",
    "\n",
    "    input_word_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1] * len(input_word_ids)\n",
    "    input_type_ids = [0] * (\n",
    "        1 + len(quest_tokens) + 1) + [1] * (len(refer_tokens) + 1)\n",
    "\n",
    "    input_word_ids, input_mask, input_type_ids = map(\n",
    "        lambda t: tf.expand_dims(\n",
    "            tf.convert_to_tensor(t, dtype=tf.int32), 0),\n",
    "        (input_word_ids, input_mask, input_type_ids))\n",
    "\n",
    "    outputs = model([input_word_ids, input_mask, input_type_ids])\n",
    "\n",
    "    short_start = tf.argmax(outputs[0][0][1:]) + 1\n",
    "    short_end = tf.argmax(outputs[1][0][1:]) + 1\n",
    "    answer_tokens = tokens[short_start: short_end + 1]\n",
    "    answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "\n",
    "    if answer is None or answer is \"\" or question in answer:\n",
    "        return None\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "with open('drive/MyDrive/ZendeskArticles/PeerLearningDays.md') as f:\n",
    "    reference = f.read()\n",
    "\n",
    "answer_loop(reference)\n",
    "     \n",
    "Q: When are PLDs?\n",
    "A:  on - site days from 9 : 00 am to 3 : 00 pm\n",
    "Q: What are Mock Interviews?\n",
    "A: Sorry, I do not understand your question.\n",
    "Q: What does PLD stand for?\n",
    "A:  peer learning days\n",
    "Q: bye\n",
    "A: Goodbye\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "def semantic_search(corpus_path, sentence):\n",
    "    \"\"\"                                                                                                                 \n",
    "    Performs semantic search on a corpus of documents                                                                   \n",
    "                                                                                                                        \n",
    "    parameters:                                                                                                         \n",
    "        corpus_path [string]:                                                                                           \n",
    "            the path to the corpus of reference documents on which                                                      \n",
    "                to perform semantic search                                                                              \n",
    "        sentence [string]:                                                                                              \n",
    "            the sentence from which to perform semantic search                                                          \n",
    "                                                                                                                        \n",
    "    returns:                                                                                                            \n",
    "        [string]:                                                                                                       \n",
    "            the reference text of the document most similar to given sentence                                           \n",
    "    \"\"\"\n",
    "    documents = [sentence]\n",
    "\n",
    "    for filename in os.listdir(corpus_path):\n",
    "        if filename.endswith(\".md\") is False:\n",
    "            continue\n",
    "        with open(corpus_path + \"/\" + filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            documents.append(f.read())\n",
    "\n",
    "    model = hub.load(\n",
    "        \"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n",
    "\n",
    "    embeddings = model(documents)\n",
    "\n",
    "    correlation = np.inner(embeddings, embeddings)\n",
    "\n",
    "    closest = np.argmax(correlation[0, 1:])\n",
    "\n",
    "    similar = documents[closest + 1]\n",
    "\n",
    "    return similar\n",
    "\n",
    "print(semantic_search('drive/MyDrive/ZendeskArticles', 'When are PLDs?'))\n",
    "     \n",
    "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder-large/5'.\n",
    "INFO:absl:Downloaded https://tfhub.dev/google/universal-sentence-encoder-large/5, Total size: 577.10MB\n",
    "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder-large/5'.\n",
    "PLD Overview\n",
    "Peer Learning Days (PLDs) are a time for you and your peers to ensure that each of you understands the concepts you've encountered in your projects, as well as a time for everyone to collectively grow in technical, professional, and soft skills. During PLD, you will collaboratively review prior projects with a group of cohort peers.\n",
    "PLD Basics\n",
    "PLDs are mandatory on-site days from 9:00 AM to 3:00 PM. If you cannot be present or on time, you must use a PTO. \n",
    "No laptops, tablets, or screens are allowed until all tasks have been whiteboarded and understood by the entirety of your group. This time is for whiteboarding, dialogue, and active peer collaboration. After this, you may return to computers with each other to pair or group program. \n",
    "Peer Learning Days are not about sharing solutions. This doesn't empower peers with the ability to solve problems themselves! Peer learning is when you share your thought process, whether through conversation, whiteboarding, debugging, or live coding. \n",
    "When a peer has a question, rather than offering the solution, ask the following:\n",
    "\"How did you come to that conclusion?\"\n",
    "\"What have you tried?\"\n",
    "\"Did the man page give you a lead?\"\n",
    "\"Did you think about this concept?\"\n",
    "Modeling this form of thinking for one another is invaluable and will strengthen your entire cohort.\n",
    "Your ability to articulate your knowledge is a crucial skill and will be required to succeed during technical interviews and through your career. \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "def question_answer(corpus_path):\n",
    "    \"\"\"                                                                                        \n",
    "    Answers questions from multiple reference texts                                            \n",
    "                                                                                               \n",
    "    parameters:                                                                                \n",
    "        corpus_path [string]:                                                                  \n",
    "            the path to the corpus of reference documents                                      \n",
    "    \"\"\"\n",
    "    while (1):\n",
    "        user_input = input(\"Q: \")\n",
    "        user_input = user_input.lower()\n",
    "        if user_input == 'exit' or user_input == 'quit' \\\n",
    "           or user_input == 'goodbye' or user_input == 'bye':\n",
    "            print(\"A: Goodbye\")\n",
    "            break\n",
    "        reference = semantic_search(corpus_path, user_input)\n",
    "        answer = specific_question_answer(user_input, reference)\n",
    "        if answer is None:\n",
    "            print(\"A: Sorry, I do not understand your question.\")\n",
    "        else:\n",
    "            print(\"A: \", answer)\n",
    "\n",
    "def semantic_search(corpus_path, sentence):\n",
    "    \"\"\"                                                                                        \n",
    "    Performs semantic search on a corpus of documents                                          \n",
    "                                                                                               \n",
    "    parameters:                                                                                \n",
    "        corpus_path [string]:                                                                  \n",
    "            the path to the corpus of reference documents on which                             \n",
    "                to perform semantic search                                                     \n",
    "        sentence [string]:                                                                     \n",
    "            the sentence from which to perform semantic search                                 \n",
    "                                                                                               \n",
    "    returns:                                                                                   \n",
    "        [string]:                                                                              \n",
    "            the reference text of the document most similar to given sentence                  \n",
    "    \"\"\"\n",
    "    documents = [sentence]\n",
    "\n",
    "    for filename in os.listdir(corpus_path):\n",
    "        if filename.endswith(\".md\") is False:\n",
    "            continue\n",
    "        with open(corpus_path + \"/\" + filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            documents.append(f.read())\n",
    "\n",
    "    model = hub.load(\n",
    "        \"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n",
    "\n",
    "    embeddings = model(documents)\n",
    "    correlation = np.inner(embeddings, embeddings)\n",
    "    closest = np.argmax(correlation[0, 1:])\n",
    "    similar = documents[closest + 1]\n",
    "\n",
    "    return similar\n",
    "\n",
    "def specific_question_answer(question, reference):\n",
    "    \"\"\"                                                                                        \n",
    "    Finds a snippet of text within a reference document to answer a question                   \n",
    "                                                                                               \n",
    "    parameters:                                                                                \n",
    "        question [string]:                                                                     \n",
    "            contains the question to answer                                                    \n",
    "        reference [string]:                                                                    \n",
    "            contains the reference document from which to find the answer                      \n",
    "                                                                                               \n",
    "    returns:                                                                                   \n",
    "        [string]:                                                                              \n",
    "            contains the answer                                                                \n",
    "        or None if no answer is found                                                          \n",
    "    \"\"\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(\n",
    "        'bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "    model = hub.load(\"https://tfhub.dev/see--/bert-uncased-tf2-qa/1\")\n",
    "\n",
    "    quest_tokens = tokenizer.tokenize(question)\n",
    "    refer_tokens = tokenizer.tokenize(reference)\n",
    "\n",
    "    tokens = ['[CLS]'] + quest_tokens + ['[SEP]'] + refer_tokens + ['[SEP]']\n",
    "\n",
    "    input_word_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1] * len(input_word_ids)\n",
    "    input_type_ids = [0] * (\n",
    "        1 + len(quest_tokens) + 1) + [1] * (len(refer_tokens) + 1)\n",
    "\n",
    "    input_word_ids, input_mask, input_type_ids = map(\n",
    "        lambda t: tf.expand_dims(\n",
    "            tf.convert_to_tensor(t, dtype=tf.int32), 0),\n",
    "        (input_word_ids, input_mask, input_type_ids))\n",
    "\n",
    "    outputs = model([input_word_ids, input_mask, input_type_ids])\n",
    "\n",
    "    short_start = tf.argmax(outputs[0][0][1:]) + 1\n",
    "    short_end = tf.argmax(outputs[1][0][1:]) + 1\n",
    "    answer_tokens = tokens[short_start: short_end + 1]\n",
    "    answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "                                                                                               \n",
    "    if answer is None or answer is \"\" or question in answer:                                   \n",
    "        return None                                                                            \n",
    "                                                                                               \n",
    "    return answer\n",
    "\n",
    "question_answer('drive/MyDrive/ZendeskArticles')\n",
    "\n",
    "     \n",
    "Q: When are PLDs?\n",
    "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n",
    "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder-large/5'.\n",
    "INFO:absl:Downloaded https://tfhub.dev/google/universal-sentence-encoder-large/5, Total size: 577.10MB\n",
    "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder-large/5'.\n",
    "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…\n",
    "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…\n",
    "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…\n",
    "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/see--/bert-uncased-tf2-qa/1'.\n",
    "INFO:absl:Downloading https://tfhub.dev/see--/bert-uncased-tf2-qa/1: 544.01MB\n",
    "INFO:absl:Downloading https://tfhub.dev/see--/bert-uncased-tf2-qa/1: 1.14GB\n",
    "INFO:absl:Downloaded https://tfhub.dev/see--/bert-uncased-tf2-qa/1, Total size: 1.27GB\n",
    "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/see--/bert-uncased-tf2-qa/1'.\n",
    "A:  on - site days from 9 : 00 am to 3 : 00 pm\n",
    "Q: What are Mock Interviews?\n",
    "A:  help you train for technical interviews\n",
    "Q: What does PLD stand for?\n",
    "A:  peer learning days\n",
    "Q: What is potato?\n",
    "A:  virtual groups of about 8 students , automatically created , based on a short quiz on study / work habits\n",
    "Q: What is jkjhrjkhrhkh?\n",
    "A: Sorry, I do not understand your question.\n",
    "Q: What does the Chief Kitchen Officer do?\n",
    "A:  to make sure that the kitchen is maintained to a high standard\n",
    "Q: exit\n",
    "A: Goodbye"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
